{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\Ex}{\\mathbb{E}}\n",
    "\\newcommand{\\Var}{\\mathrm{Var}}\n",
    "\\newcommand{\\Cov}{\\mathrm{Cov}}\n",
    "\\newcommand{\\SampleAvg}{\\frac{1}{N({S})} \\sum_{s \\in {S}}}\n",
    "\\newcommand{\\indic}{\\mathbb{1}}\n",
    "\\newcommand{\\avg}{\\overline}\n",
    "\\newcommand{\\est}{\\hat}\n",
    "\\newcommand{\\trueval}[1]{#1^{*}}\n",
    "\\newcommand{\\Gam}[1]{\\mathrm{Gamma}#1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\renewcommand{\\like}{\\cal L}\n",
    "\\renewcommand{\\loglike}{\\ell}\n",
    "\\renewcommand{\\err}{\\cal E}\n",
    "\\renewcommand{\\dat}{\\cal D}\n",
    "\\renewcommand{\\hyp}{\\cal H}\n",
    "\\renewcommand{\\Ex}[2]{E_{#1}[#2]}\n",
    "\\renewcommand{\\x}{\\mathbf x}\n",
    "\\renewcommand{\\v}[1]{\\mathbf #1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a parametric model\n",
    "\n",
    "When we do data analysis in a parametric way, we start by characterizing our particular sample statistically then, using a *probability distribution* (or mass function). This distribution has some parameters. Lets refer to these as $\\theta$.\n",
    "\n",
    "If we assume that our **data was generated** by this distribution, then the notion of the **true value** of the parameter makes sense. Now, usually in life, there is no way of knowing if this was the true generating process, unless we have some physics or similar ideas behind the process. But lets stick with the *myth* that we can do this. Then let us call the true value of the parameters as $\\theta^*$.\n",
    "\n",
    "To know this true value, we'd typically need the entire large population, not the sample we have been given as data. So the best we can do us to make a parameter estimate $\\hat{\\theta}$ from the data. In the context of frequentist statistics, the assumption is that the parameters are fixed, and that there is this true value ($\\theta^*$), and that we can make some estimate of this from our sample ($\\hat{\\theta}$).\n",
    "\n",
    "A distribution is induced on this estimate by considering many samples that could have been drawn from the population...remember that frequentist statistics fixes the parameters but considers data stochastic. This distribution is called the sampling distribution of the parameter $\\theta$. (In general a sampling distribution can be considered for anything computed on the sample, such as a mean or variance or other moment).\n",
    "\n",
    "Our question is: how do we estimate  $\\hat{\\theta}$. And how do we compute this sampling distribution so that we can get a notion of the uncertainty that estimating from a sample rather than the population leaves us with?\n",
    "\n",
    "The first question is tackled by the Maximum Likelihood estimate, or MLE. The second one is tackled by techniques like the bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The idea behind the MLE\n",
    "\n",
    "The diagram below  illustrates the idea behind the MLE.\n",
    "\n",
    "![](images/gaussmle.png)\n",
    "\n",
    "Consider two distributions in the same family, one with a parameter, lets call it $\\theta$, of value 1.8 (blue) and another of value 5.8. (green). Let's say we have 3 data points, at $x=1,2,3$.\n",
    "\n",
    "Maximum likelihood starts by asking the question: conditional on the fixed value of $\\theta$, which distribution is the data more likely to have come from?\n",
    "\n",
    "In our case the blue is more likely since the product of the height of the 3 vertical blue bars is higher than that of the 3 green bars.\n",
    "\n",
    "Indeed the question that MLE asks is: how can we move and scale the distribution, that is, change $\\theta$, until the product of the 3 bars is maximised!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, the product \n",
    "\n",
    "$$\n",
    "L(\\lambda) = \\prod_{i=1}^n P(x_i \\mid \\lambda)\n",
    "$$\n",
    "\n",
    "gives us a measure of how likely it is to observe values $x_1,...,x_n$ given the parameters $\\lambda$. Maximum likelihood fitting consists of choosing the appropriate \"likelihood\" function $L=P(X \\mid \\lambda)$ to maximize for a given set of observations. How likely are the observations if the model is true?\n",
    "\n",
    "Often it is easier and numerically more stable to maximise the log likelyhood:\n",
    "\n",
    "$$\n",
    "\\ell(\\lambda) = \\sum_{i=1}^n ln(P(x_i \\mid \\lambda))\n",
    "$$\n",
    "\n",
    "The exponential distribution occurs naturally when describing the lengths of the inter-arrival times in a homogeneous Poisson process.\n",
    "\n",
    "It takes the form:\n",
    "$$\n",
    "f(x;\\lambda) = \\begin{cases}\n",
    "\\lambda e^{-\\lambda x} & x \\ge 0, \\\\\n",
    "0 & x < 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In the case of the exponential distribution we have:\n",
    "\n",
    "$$\n",
    "\\ell(lambda) = \\sum_{i=1}^n ln(\\lambda e^{-\\lambda x_i}) = \\sum_{i=1}^n \\left( ln(\\lambda) - \\lambda x_i \\right).\n",
    "$$\n",
    "\n",
    "Maximizing this:\n",
    "\n",
    "$$\n",
    "\\frac{d \\ell}{d\\lambda} = \\frac{n}{\\lambda} - \\sum_{i=1}^n x_i = 0\n",
    "$$\n",
    "\n",
    "and thus:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\est{\\lambda_{MLE}}} = \\frac{1}{n}\\sum_{i=1}^n x_i,\n",
    "$$\n",
    "\n",
    "which is the sample mean of our sample. Usually one is not so lucky and one must use numerical optimization techniques.\n",
    "\n",
    "A crucial property is that, for many commonly occurring situations, maximum likelihood parameter estimators have an approximate normal distribution when n is large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference \n",
    "\n",
    "Just having an estimate is no good. We will want to put confidence intervals on the estimation of the parameters. This presents a conundrum: we have access to only one sample, but want to compute a error estimate over multiple samples, using an estimator such as the standard deviation.\n",
    "\n",
    "At this point we are wishing for the Lord  to have given us other samples drawn from the population. But alas, no such luck...\n",
    "\n",
    "So how then are we to find the sampling distribution of our parameters?\n",
    "\n",
    "In the last two decades, **resampling** the ONE dataset we have has become computationally feasible. Resampling involves making new samples from the observations, each of which is analysed in the same way as out original dataset. One way to do this is the Bootstrap. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Linear Regression MLE\n",
    "\n",
    "Linear regression is the workhorse algorithm thats used in many sciences, social and natural. The diagram below illustrates the probabilistic interpretation of linear regression, and the idea behind the MLE for linear regression. We illustrate a point $(x_i, y_i)$, and the corresponding prediction  for $x_i$ using the line, that is $yhat_i$ or $\\hat{y}_i$.\n",
    "\n",
    "![](images/linregmle.png)\n",
    "\n",
    "The fundamental assumption for the probabilistic analysis of linear regression is that each $y_i$ is gaussian distributed with mean  $\\v{w}\\cdot\\v{x_i}$ (the y predicted by the regression line so to speak) and variance $\\sigma^2$:\n",
    "\n",
    "$$ y_i \\sim N(\\v{w}\\cdot\\v{x_i}, \\sigma^2) .$$\n",
    "\n",
    "We can then write the likelihood:\n",
    "\n",
    "$$\\cal{L} = p(\\v{y} | \\v{x}, \\v{w}, \\sigma) = \\prod_i p(\\v{y}_i | \\v{x}_i, \\v{w}, \\sigma)$$\n",
    "\n",
    "Given the canonical form of the gaussian:\n",
    "\n",
    "$$N(\\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-(y - \\mu)^2 / 2\\sigma^2},$$\n",
    "\n",
    "we can show that:\n",
    "\n",
    "$$\\cal{L} =  (2\\pi\\sigma^2)^{(-n/2)} e^{\\frac{-1}{2\\sigma^2} \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2} .$$\n",
    "\n",
    "The log likelihood $\\ell$ then is given by:\n",
    "\n",
    "$$\\ell = \\frac{-n}{2} log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}  \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2 .$$\n",
    "\n",
    "If you differentiate this with respect to  $\\v{w}$ and $\\sigma$, you get the MLE values of the parameter estimates:\n",
    "\n",
    "$$\\v{w}_{MLE} = (\\v{X}^T\\v{X})^{-1} \\v{X}^T\\v{y}, $$\n",
    "\n",
    "where $\\v{X}$ is the design matrix created by stacking rows $\\v{x}_i$, and\n",
    "\n",
    "$$\\sigma^2_{MLE} =  \\frac{1}{n} \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2  . $$\n",
    "\n",
    "These are the standard results of linear regression."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
